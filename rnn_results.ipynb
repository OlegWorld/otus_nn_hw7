{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as ds\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('../')\n",
    "# import utils\n",
    "# import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = lambda x: list(x)\n",
    "TEXT = data.Field(sequential=True, tokenize=token)\n",
    "\n",
    "train = ds.LanguageModelingDataset(path='wikitext/train.txt', text_field=TEXT)\n",
    "valid = ds.LanguageModelingDataset(path='wikitext/valid.txt', text_field=TEXT)\n",
    "test = ds.LanguageModelingDataset(path='wikitext/test.txt', text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = wiki_utils.Texts('./wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 128\n",
    "train_loader, val_loader, test_loader = data.BPTTIterator.splits((train, valid, test),\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             bptt_len=sequence_length,\n",
    "                                                             repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_batch_size = 128\n",
    "# train_loader = wiki_utils.TextLoader(corpus.train, batch_size=batch_size)\n",
    "# val_loader = wiki_utils.TextLoader(corpus.valid, batch_size=eval_batch_size)\n",
    "# test_loader = wiki_utils.TextLoader(corpus.test, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i, data in enumerate(data_loader):\n",
    "        output, hidden = model(data.text)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data.text) * criterion(output_flat, data.target.view(-1)).item()\n",
    "    return total_loss / math.ceil(len(data_loader.dataset[0].text) / data_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    for batch, data in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data.text)\n",
    "        loss = criterion(output.view(-1, ntokens), data.target.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "#         s = corpus.dictionary.idx2symbol[s_idx]\n",
    "        s = TEXT.vocab.itos[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ・Á†Đ\"`უÅễ\\უāaīAèçá$OsLKáκลგ<unk>Ł(E)アัøァI²$ăòíĀt^์ìdგต \n",
      "\n",
      "| epoch   1 |   100/   93 batches | lr 4.00 | loss  3.59 | ppl    36.13\n",
      "| epoch   1 |   200/   93 batches | lr 4.00 | loss  3.28 | ppl    26.64\n",
      "| epoch   1 |   300/   93 batches | lr 4.00 | loss  3.25 | ppl    25.80\n",
      "| epoch   1 |   400/   93 batches | lr 4.00 | loss  3.22 | ppl    25.10\n",
      "| epoch   1 |   500/   93 batches | lr 4.00 | loss  3.20 | ppl    24.49\n",
      "| epoch   1 |   600/   93 batches | lr 4.00 | loss  3.07 | ppl    21.55\n",
      "| epoch   1 |   700/   93 batches | lr 4.00 | loss  2.97 | ppl    19.54\n",
      "| epoch   1 |   800/   93 batches | lr 4.00 | loss  2.88 | ppl    17.83\n",
      "| epoch   1 |   900/   93 batches | lr 4.00 | loss  2.80 | ppl    16.48\n",
      "| epoch   1 |  1000/   93 batches | lr 4.00 | loss  2.73 | ppl    15.33\n",
      "| epoch   1 |  1100/   93 batches | lr 4.00 | loss  2.63 | ppl    13.94\n",
      "| epoch   1 |  1200/   93 batches | lr 4.00 | loss  2.57 | ppl    13.04\n",
      "| epoch   1 |  1300/   93 batches | lr 4.00 | loss  2.51 | ppl    12.32\n",
      "| epoch   1 |  1400/   93 batches | lr 4.00 | loss  2.45 | ppl    11.64\n",
      "| epoch   1 |  1500/   93 batches | lr 4.00 | loss  2.42 | ppl    11.23\n",
      "| epoch   1 |  1600/   93 batches | lr 4.00 | loss  2.38 | ppl    10.82\n",
      "| epoch   1 |  1700/   93 batches | lr 4.00 | loss  2.34 | ppl    10.41\n",
      "| epoch   1 |  1800/   93 batches | lr 4.00 | loss  2.31 | ppl    10.10\n",
      "| epoch   1 |  1900/   93 batches | lr 4.00 | loss  2.29 | ppl     9.84\n",
      "| epoch   1 |  2000/   93 batches | lr 4.00 | loss  2.26 | ppl     9.55\n",
      "| epoch   1 |  2100/   93 batches | lr 4.00 | loss  2.24 | ppl     9.36\n",
      "| epoch   1 |  2200/   93 batches | lr 4.00 | loss  2.21 | ppl     9.16\n",
      "| epoch   1 |  2300/   93 batches | lr 4.00 | loss  2.20 | ppl     9.06\n",
      "| epoch   1 |  2400/   93 batches | lr 4.00 | loss  2.18 | ppl     8.82\n",
      "| epoch   1 |  2500/   93 batches | lr 4.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   1 |  2600/   93 batches | lr 4.00 | loss  2.15 | ppl     8.57\n",
      "| epoch   1 |  2700/   93 batches | lr 4.00 | loss  2.13 | ppl     8.44\n",
      "| epoch   1 |  2800/   93 batches | lr 4.00 | loss  2.11 | ppl     8.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.93 | valid ppl     6.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  y . <eos> <eos> <eos> Anle thee for galy and ariemed loded in \n",
      "\n",
      "| epoch   2 |   100/   93 batches | lr 1.00 | loss  2.11 | ppl     8.28\n",
      "| epoch   2 |   200/   93 batches | lr 1.00 | loss  2.08 | ppl     8.01\n",
      "| epoch   2 |   300/   93 batches | lr 1.00 | loss  2.08 | ppl     8.00\n",
      "| epoch   2 |   400/   93 batches | lr 1.00 | loss  2.08 | ppl     7.97\n",
      "| epoch   2 |   500/   93 batches | lr 1.00 | loss  2.07 | ppl     7.94\n",
      "| epoch   2 |   600/   93 batches | lr 1.00 | loss  2.07 | ppl     7.90\n",
      "| epoch   2 |   700/   93 batches | lr 1.00 | loss  2.06 | ppl     7.88\n",
      "| epoch   2 |   800/   93 batches | lr 1.00 | loss  2.06 | ppl     7.85\n",
      "| epoch   2 |   900/   93 batches | lr 1.00 | loss  2.06 | ppl     7.86\n",
      "| epoch   2 |  1000/   93 batches | lr 1.00 | loss  2.06 | ppl     7.84\n",
      "| epoch   2 |  1100/   93 batches | lr 1.00 | loss  2.05 | ppl     7.75\n",
      "| epoch   2 |  1200/   93 batches | lr 1.00 | loss  2.05 | ppl     7.74\n",
      "| epoch   2 |  1300/   93 batches | lr 1.00 | loss  2.04 | ppl     7.72\n",
      "| epoch   2 |  1400/   93 batches | lr 1.00 | loss  2.03 | ppl     7.64\n",
      "| epoch   2 |  1500/   93 batches | lr 1.00 | loss  2.04 | ppl     7.67\n",
      "| epoch   2 |  1600/   93 batches | lr 1.00 | loss  2.03 | ppl     7.65\n",
      "| epoch   2 |  1700/   93 batches | lr 1.00 | loss  2.03 | ppl     7.62\n",
      "| epoch   2 |  1800/   93 batches | lr 1.00 | loss  2.03 | ppl     7.59\n",
      "| epoch   2 |  1900/   93 batches | lr 1.00 | loss  2.03 | ppl     7.60\n",
      "| epoch   2 |  2000/   93 batches | lr 1.00 | loss  2.02 | ppl     7.53\n",
      "| epoch   2 |  2100/   93 batches | lr 1.00 | loss  2.02 | ppl     7.55\n",
      "| epoch   2 |  2200/   93 batches | lr 1.00 | loss  2.02 | ppl     7.51\n",
      "| epoch   2 |  2300/   93 batches | lr 1.00 | loss  2.02 | ppl     7.55\n",
      "| epoch   2 |  2400/   93 batches | lr 1.00 | loss  2.01 | ppl     7.44\n",
      "| epoch   2 |  2500/   93 batches | lr 1.00 | loss  2.01 | ppl     7.44\n",
      "| epoch   2 |  2600/   93 batches | lr 1.00 | loss  2.01 | ppl     7.45\n",
      "| epoch   2 |  2700/   93 batches | lr 1.00 | loss  2.00 | ppl     7.41\n",
      "| epoch   2 |  2800/   93 batches | lr 1.00 | loss  1.99 | ppl     7.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.81 | valid ppl     6.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  hime to the peen and reomated lgads coll was the  \n",
      "\n",
      "| epoch   3 |   100/   93 batches | lr 1.00 | loss  2.01 | ppl     7.49\n",
      "| epoch   3 |   200/   93 batches | lr 1.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   3 |   300/   93 batches | lr 1.00 | loss  1.99 | ppl     7.28\n",
      "| epoch   3 |   400/   93 batches | lr 1.00 | loss  1.98 | ppl     7.27\n",
      "| epoch   3 |   500/   93 batches | lr 1.00 | loss  1.98 | ppl     7.24\n",
      "| epoch   3 |   600/   93 batches | lr 1.00 | loss  1.98 | ppl     7.22\n",
      "| epoch   3 |   700/   93 batches | lr 1.00 | loss  1.98 | ppl     7.23\n",
      "| epoch   3 |   800/   93 batches | lr 1.00 | loss  1.97 | ppl     7.20\n",
      "| epoch   3 |   900/   93 batches | lr 1.00 | loss  1.98 | ppl     7.21\n",
      "| epoch   3 |  1000/   93 batches | lr 1.00 | loss  1.97 | ppl     7.20\n",
      "| epoch   3 |  1100/   93 batches | lr 1.00 | loss  1.97 | ppl     7.14\n",
      "| epoch   3 |  1200/   93 batches | lr 1.00 | loss  1.97 | ppl     7.16\n",
      "| epoch   3 |  1300/   93 batches | lr 1.00 | loss  1.96 | ppl     7.13\n",
      "| epoch   3 |  1400/   93 batches | lr 1.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   3 |  1500/   93 batches | lr 1.00 | loss  1.96 | ppl     7.10\n",
      "| epoch   3 |  1600/   93 batches | lr 1.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   3 |  1700/   93 batches | lr 1.00 | loss  1.96 | ppl     7.07\n",
      "| epoch   3 |  1800/   93 batches | lr 1.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   3 |  1900/   93 batches | lr 1.00 | loss  1.96 | ppl     7.09\n",
      "| epoch   3 |  2000/   93 batches | lr 1.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   3 |  2100/   93 batches | lr 1.00 | loss  1.95 | ppl     7.05\n",
      "| epoch   3 |  2200/   93 batches | lr 1.00 | loss  1.95 | ppl     7.02\n",
      "| epoch   3 |  2300/   93 batches | lr 1.00 | loss  1.95 | ppl     7.06\n",
      "| epoch   3 |  2400/   93 batches | lr 1.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   3 |  2500/   93 batches | lr 1.00 | loss  1.94 | ppl     6.96\n",
      "| epoch   3 |  2600/   93 batches | lr 1.00 | loss  1.94 | ppl     6.99\n",
      "| epoch   3 |  2700/   93 batches | lr 1.00 | loss  1.94 | ppl     6.97\n",
      "| epoch   3 |  2800/   93 batches | lr 1.00 | loss  1.93 | ppl     6.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.74 | valid ppl     5.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " er Goundes nider menented , and stack and <unk> ry \n",
      "\n",
      "| epoch   4 |   100/   93 batches | lr 1.00 | loss  1.95 | ppl     7.04\n",
      "| epoch   4 |   200/   93 batches | lr 1.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   4 |   300/   93 batches | lr 1.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   4 |   400/   93 batches | lr 1.00 | loss  1.93 | ppl     6.86\n",
      "| epoch   4 |   500/   93 batches | lr 1.00 | loss  1.92 | ppl     6.84\n",
      "| epoch   4 |   600/   93 batches | lr 1.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   4 |   700/   93 batches | lr 1.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   4 |   800/   93 batches | lr 1.00 | loss  1.92 | ppl     6.81\n",
      "| epoch   4 |   900/   93 batches | lr 1.00 | loss  1.92 | ppl     6.82\n",
      "| epoch   4 |  1000/   93 batches | lr 1.00 | loss  1.92 | ppl     6.83\n",
      "| epoch   4 |  1100/   93 batches | lr 1.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   4 |  1200/   93 batches | lr 1.00 | loss  1.92 | ppl     6.79\n",
      "| epoch   4 |  1300/   93 batches | lr 1.00 | loss  1.91 | ppl     6.77\n",
      "| epoch   4 |  1400/   93 batches | lr 1.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   4 |  1500/   93 batches | lr 1.00 | loss  1.91 | ppl     6.73\n",
      "| epoch   4 |  1600/   93 batches | lr 1.00 | loss  1.91 | ppl     6.75\n",
      "| epoch   4 |  1700/   93 batches | lr 1.00 | loss  1.91 | ppl     6.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/   93 batches | lr 1.00 | loss  1.90 | ppl     6.71\n",
      "| epoch   4 |  1900/   93 batches | lr 1.00 | loss  1.91 | ppl     6.76\n",
      "| epoch   4 |  2000/   93 batches | lr 1.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   4 |  2100/   93 batches | lr 1.00 | loss  1.91 | ppl     6.73\n",
      "| epoch   4 |  2200/   93 batches | lr 1.00 | loss  1.90 | ppl     6.70\n",
      "| epoch   4 |  2300/   93 batches | lr 1.00 | loss  1.91 | ppl     6.74\n",
      "| epoch   4 |  2400/   93 batches | lr 1.00 | loss  1.90 | ppl     6.66\n",
      "| epoch   4 |  2500/   93 batches | lr 1.00 | loss  1.90 | ppl     6.66\n",
      "| epoch   4 |  2600/   93 batches | lr 1.00 | loss  1.90 | ppl     6.69\n",
      "| epoch   4 |  2700/   93 batches | lr 1.00 | loss  1.90 | ppl     6.68\n",
      "| epoch   4 |  2800/   93 batches | lr 1.00 | loss  1.88 | ppl     6.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.69 | valid ppl     5.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  1952 coneuther hest about Endarul whace the Mey . \n",
      "\n",
      "| epoch   5 |   100/   93 batches | lr 1.00 | loss  1.91 | ppl     6.74\n",
      "| epoch   5 |   200/   93 batches | lr 1.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   5 |   300/   93 batches | lr 1.00 | loss  1.88 | ppl     6.58\n",
      "| epoch   5 |   400/   93 batches | lr 1.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   5 |   500/   93 batches | lr 1.00 | loss  1.88 | ppl     6.56\n",
      "| epoch   5 |   600/   93 batches | lr 1.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   5 |   700/   93 batches | lr 1.00 | loss  1.88 | ppl     6.55\n",
      "| epoch   5 |   800/   93 batches | lr 1.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   5 |   900/   93 batches | lr 1.00 | loss  1.88 | ppl     6.56\n",
      "| epoch   5 |  1000/   93 batches | lr 1.00 | loss  1.88 | ppl     6.57\n",
      "| epoch   5 |  1100/   93 batches | lr 1.00 | loss  1.87 | ppl     6.52\n",
      "| epoch   5 |  1200/   93 batches | lr 1.00 | loss  1.88 | ppl     6.54\n",
      "| epoch   5 |  1300/   93 batches | lr 1.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   5 |  1400/   93 batches | lr 1.00 | loss  1.86 | ppl     6.45\n",
      "| epoch   5 |  1500/   93 batches | lr 1.00 | loss  1.87 | ppl     6.49\n",
      "| epoch   5 |  1600/   93 batches | lr 1.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   5 |  1700/   93 batches | lr 1.00 | loss  1.87 | ppl     6.48\n",
      "| epoch   5 |  1800/   93 batches | lr 1.00 | loss  1.87 | ppl     6.49\n",
      "| epoch   5 |  1900/   93 batches | lr 1.00 | loss  1.88 | ppl     6.53\n",
      "| epoch   5 |  2000/   93 batches | lr 1.00 | loss  1.87 | ppl     6.46\n",
      "| epoch   5 |  2100/   93 batches | lr 1.00 | loss  1.87 | ppl     6.50\n",
      "| epoch   5 |  2200/   93 batches | lr 1.00 | loss  1.87 | ppl     6.47\n",
      "| epoch   5 |  2300/   93 batches | lr 1.00 | loss  1.87 | ppl     6.51\n",
      "| epoch   5 |  2400/   93 batches | lr 1.00 | loss  1.86 | ppl     6.44\n",
      "| epoch   5 |  2500/   93 batches | lr 1.00 | loss  1.86 | ppl     6.44\n",
      "| epoch   5 |  2600/   93 batches | lr 1.00 | loss  1.87 | ppl     6.47\n",
      "| epoch   5 |  2700/   93 batches | lr 1.00 | loss  1.86 | ppl     6.46\n",
      "| epoch   5 |  2800/   93 batches | lr 1.00 | loss  1.85 | ppl     6.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.65 | valid ppl     5.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  his Pocrel <unk> that on at of Nildaws 's season  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.encoder(x)\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab)\n",
    "model = RNNModel('GRU', ntokens, 128, 128, 2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " ₹Сァơキ〉ệβ;ưกтử+ầṃ°^,iჯÜヴცาđ攻F/łвÉ)ḥÆéḥ9ỹDLя*îJ大å空ä \n",
      "\n",
      "| epoch   1 |   100/   93 batches | lr 1.00 | loss  3.84 | ppl    46.36\n",
      "| epoch   1 |   200/   93 batches | lr 1.00 | loss  3.27 | ppl    26.27\n",
      "| epoch   1 |   300/   93 batches | lr 1.00 | loss  3.24 | ppl    25.56\n",
      "| epoch   1 |   400/   93 batches | lr 1.00 | loss  3.22 | ppl    25.01\n",
      "| epoch   1 |   500/   93 batches | lr 1.00 | loss  3.22 | ppl    25.11\n",
      "| epoch   1 |   600/   93 batches | lr 1.00 | loss  3.21 | ppl    24.81\n",
      "| epoch   1 |   700/   93 batches | lr 1.00 | loss  3.21 | ppl    24.66\n",
      "| epoch   1 |   800/   93 batches | lr 1.00 | loss  3.17 | ppl    23.92\n",
      "| epoch   1 |   900/   93 batches | lr 1.00 | loss  3.12 | ppl    22.71\n",
      "| epoch   1 |  1000/   93 batches | lr 1.00 | loss  3.06 | ppl    21.26\n",
      "| epoch   1 |  1100/   93 batches | lr 1.00 | loss  2.98 | ppl    19.75\n",
      "| epoch   1 |  1200/   93 batches | lr 1.00 | loss  2.93 | ppl    18.79\n",
      "| epoch   1 |  1300/   93 batches | lr 1.00 | loss  2.90 | ppl    18.19\n",
      "| epoch   1 |  1400/   93 batches | lr 1.00 | loss  2.86 | ppl    17.47\n",
      "| epoch   1 |  1500/   93 batches | lr 1.00 | loss  2.83 | ppl    16.93\n",
      "| epoch   1 |  1600/   93 batches | lr 1.00 | loss  2.80 | ppl    16.38\n",
      "| epoch   1 |  1700/   93 batches | lr 1.00 | loss  2.74 | ppl    15.56\n",
      "| epoch   1 |  1800/   93 batches | lr 1.00 | loss  2.70 | ppl    14.94\n",
      "| epoch   1 |  1900/   93 batches | lr 1.00 | loss  2.68 | ppl    14.59\n",
      "| epoch   1 |  2000/   93 batches | lr 1.00 | loss  2.65 | ppl    14.19\n",
      "| epoch   1 |  2100/   93 batches | lr 1.00 | loss  2.63 | ppl    13.91\n",
      "| epoch   1 |  2200/   93 batches | lr 1.00 | loss  2.61 | ppl    13.60\n",
      "| epoch   1 |  2300/   93 batches | lr 1.00 | loss  2.61 | ppl    13.56\n",
      "| epoch   1 |  2400/   93 batches | lr 1.00 | loss  2.58 | ppl    13.17\n",
      "| epoch   1 |  2500/   93 batches | lr 1.00 | loss  2.57 | ppl    13.01\n",
      "| epoch   1 |  2600/   93 batches | lr 1.00 | loss  2.53 | ppl    12.59\n",
      "| epoch   1 |  2700/   93 batches | lr 1.00 | loss  2.51 | ppl    12.35\n",
      "| epoch   1 |  2800/   93 batches | lr 1.00 | loss  2.49 | ppl    12.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  2.43 | valid ppl    11.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " sawsedot terettins <fatetedrt afd tuoncerigoras 3o \n",
      "\n",
      "| epoch   2 |   100/   93 batches | lr 0.25 | loss  2.49 | ppl    12.07\n",
      "| epoch   2 |   200/   93 batches | lr 0.25 | loss  2.44 | ppl    11.44\n",
      "| epoch   2 |   300/   93 batches | lr 0.25 | loss  2.43 | ppl    11.31\n",
      "| epoch   2 |   400/   93 batches | lr 0.25 | loss  2.41 | ppl    11.10\n",
      "| epoch   2 |   500/   93 batches | lr 0.25 | loss  2.40 | ppl    11.00\n",
      "| epoch   2 |   600/   93 batches | lr 0.25 | loss  2.39 | ppl    10.87\n",
      "| epoch   2 |   700/   93 batches | lr 0.25 | loss  2.37 | ppl    10.73\n",
      "| epoch   2 |   800/   93 batches | lr 0.25 | loss  2.36 | ppl    10.62\n",
      "| epoch   2 |   900/   93 batches | lr 0.25 | loss  2.36 | ppl    10.60\n",
      "| epoch   2 |  1000/   93 batches | lr 0.25 | loss  2.35 | ppl    10.50\n",
      "| epoch   2 |  1100/   93 batches | lr 0.25 | loss  2.33 | ppl    10.24\n",
      "| epoch   2 |  1200/   93 batches | lr 0.25 | loss  2.32 | ppl    10.17\n",
      "| epoch   2 |  1300/   93 batches | lr 0.25 | loss  2.31 | ppl    10.12\n",
      "| epoch   2 |  1400/   93 batches | lr 0.25 | loss  2.30 | ppl    10.01\n",
      "| epoch   2 |  1500/   93 batches | lr 0.25 | loss  2.30 | ppl     9.97\n",
      "| epoch   2 |  1600/   93 batches | lr 0.25 | loss  2.29 | ppl     9.91\n",
      "| epoch   2 |  1700/   93 batches | lr 0.25 | loss  2.28 | ppl     9.80\n",
      "| epoch   2 |  1800/   93 batches | lr 0.25 | loss  2.28 | ppl     9.74\n",
      "| epoch   2 |  1900/   93 batches | lr 0.25 | loss  2.27 | ppl     9.70\n",
      "| epoch   2 |  2000/   93 batches | lr 0.25 | loss  2.26 | ppl     9.58\n",
      "| epoch   2 |  2100/   93 batches | lr 0.25 | loss  2.26 | ppl     9.54\n",
      "| epoch   2 |  2200/   93 batches | lr 0.25 | loss  2.25 | ppl     9.47\n",
      "| epoch   2 |  2300/   93 batches | lr 0.25 | loss  2.25 | ppl     9.53\n",
      "| epoch   2 |  2400/   93 batches | lr 0.25 | loss  2.24 | ppl     9.40\n",
      "| epoch   2 |  2500/   93 batches | lr 0.25 | loss  2.24 | ppl     9.44\n",
      "| epoch   2 |  2600/   93 batches | lr 0.25 | loss  2.23 | ppl     9.33\n",
      "| epoch   2 |  2700/   93 batches | lr 0.25 | loss  2.23 | ppl     9.28\n",
      "| epoch   2 |  2800/   93 batches | lr 0.25 | loss  2.21 | ppl     9.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  2.14 | valid ppl     8.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " @ 10 4-@ dire chac. idebe moucte milisk \" theald 1 \n",
      "\n",
      "| epoch   3 |   100/   93 batches | lr 0.06 | loss  2.24 | ppl     9.40\n",
      "| epoch   3 |   200/   93 batches | lr 0.06 | loss  2.21 | ppl     9.10\n",
      "| epoch   3 |   300/   93 batches | lr 0.06 | loss  2.21 | ppl     9.11\n",
      "| epoch   3 |   400/   93 batches | lr 0.06 | loss  2.21 | ppl     9.09\n",
      "| epoch   3 |   500/   93 batches | lr 0.06 | loss  2.21 | ppl     9.08\n",
      "| epoch   3 |   600/   93 batches | lr 0.06 | loss  2.21 | ppl     9.08\n",
      "| epoch   3 |   700/   93 batches | lr 0.06 | loss  2.21 | ppl     9.07\n",
      "| epoch   3 |   800/   93 batches | lr 0.06 | loss  2.21 | ppl     9.08\n",
      "| epoch   3 |   900/   93 batches | lr 0.06 | loss  2.21 | ppl     9.13\n",
      "| epoch   3 |  1000/   93 batches | lr 0.06 | loss  2.21 | ppl     9.12\n",
      "| epoch   3 |  1100/   93 batches | lr 0.06 | loss  2.20 | ppl     9.01\n",
      "| epoch   3 |  1200/   93 batches | lr 0.06 | loss  2.20 | ppl     9.02\n",
      "| epoch   3 |  1300/   93 batches | lr 0.06 | loss  2.20 | ppl     9.03\n",
      "| epoch   3 |  1400/   93 batches | lr 0.06 | loss  2.20 | ppl     8.99\n",
      "| epoch   3 |  1500/   93 batches | lr 0.06 | loss  2.20 | ppl     9.03\n",
      "| epoch   3 |  1600/   93 batches | lr 0.06 | loss  2.20 | ppl     9.02\n",
      "| epoch   3 |  1700/   93 batches | lr 0.06 | loss  2.20 | ppl     8.99\n",
      "| epoch   3 |  1800/   93 batches | lr 0.06 | loss  2.20 | ppl     8.98\n",
      "| epoch   3 |  1900/   93 batches | lr 0.06 | loss  2.20 | ppl     9.00\n",
      "| epoch   3 |  2000/   93 batches | lr 0.06 | loss  2.19 | ppl     8.92\n",
      "| epoch   3 |  2100/   93 batches | lr 0.06 | loss  2.19 | ppl     8.94\n",
      "| epoch   3 |  2200/   93 batches | lr 0.06 | loss  2.19 | ppl     8.91\n",
      "| epoch   3 |  2300/   93 batches | lr 0.06 | loss  2.20 | ppl     9.00\n",
      "| epoch   3 |  2400/   93 batches | lr 0.06 | loss  2.19 | ppl     8.92\n",
      "| epoch   3 |  2500/   93 batches | lr 0.06 | loss  2.19 | ppl     8.98\n",
      "| epoch   3 |  2600/   93 batches | lr 0.06 | loss  2.19 | ppl     8.92\n",
      "| epoch   3 |  2700/   93 batches | lr 0.06 | loss  2.19 | ppl     8.89\n",
      "| epoch   3 |  2800/   93 batches | lr 0.06 | loss  2.17 | ppl     8.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  2.10 | valid ppl     8.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  olo Dinshionclalis the Mae on umora Mase Baece Bl \n",
      "\n",
      "| epoch   4 |   100/   93 batches | lr 0.02 | loss  2.20 | ppl     9.06\n",
      "| epoch   4 |   200/   93 batches | lr 0.02 | loss  2.17 | ppl     8.79\n",
      "| epoch   4 |   300/   93 batches | lr 0.02 | loss  2.18 | ppl     8.81\n",
      "| epoch   4 |   400/   93 batches | lr 0.02 | loss  2.17 | ppl     8.80\n",
      "| epoch   4 |   500/   93 batches | lr 0.02 | loss  2.17 | ppl     8.80\n",
      "| epoch   4 |   600/   93 batches | lr 0.02 | loss  2.18 | ppl     8.81\n",
      "| epoch   4 |   700/   93 batches | lr 0.02 | loss  2.18 | ppl     8.82\n",
      "| epoch   4 |   800/   93 batches | lr 0.02 | loss  2.18 | ppl     8.82\n",
      "| epoch   4 |   900/   93 batches | lr 0.02 | loss  2.18 | ppl     8.88\n",
      "| epoch   4 |  1000/   93 batches | lr 0.02 | loss  2.18 | ppl     8.89\n",
      "| epoch   4 |  1100/   93 batches | lr 0.02 | loss  2.17 | ppl     8.79\n",
      "| epoch   4 |  1200/   93 batches | lr 0.02 | loss  2.18 | ppl     8.80\n",
      "| epoch   4 |  1300/   93 batches | lr 0.02 | loss  2.18 | ppl     8.83\n",
      "| epoch   4 |  1400/   93 batches | lr 0.02 | loss  2.17 | ppl     8.80\n",
      "| epoch   4 |  1500/   93 batches | lr 0.02 | loss  2.18 | ppl     8.85\n",
      "| epoch   4 |  1600/   93 batches | lr 0.02 | loss  2.18 | ppl     8.84\n",
      "| epoch   4 |  1700/   93 batches | lr 0.02 | loss  2.18 | ppl     8.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/   93 batches | lr 0.02 | loss  2.18 | ppl     8.82\n",
      "| epoch   4 |  1900/   93 batches | lr 0.02 | loss  2.18 | ppl     8.84\n",
      "| epoch   4 |  2000/   93 batches | lr 0.02 | loss  2.17 | ppl     8.78\n",
      "| epoch   4 |  2100/   93 batches | lr 0.02 | loss  2.18 | ppl     8.81\n",
      "| epoch   4 |  2200/   93 batches | lr 0.02 | loss  2.17 | ppl     8.79\n",
      "| epoch   4 |  2300/   93 batches | lr 0.02 | loss  2.18 | ppl     8.88\n",
      "| epoch   4 |  2400/   93 batches | lr 0.02 | loss  2.18 | ppl     8.81\n",
      "| epoch   4 |  2500/   93 batches | lr 0.02 | loss  2.18 | ppl     8.87\n",
      "| epoch   4 |  2600/   93 batches | lr 0.02 | loss  2.18 | ppl     8.82\n",
      "| epoch   4 |  2700/   93 batches | lr 0.02 | loss  2.18 | ppl     8.81\n",
      "| epoch   4 |  2800/   93 batches | lr 0.02 | loss  2.16 | ppl     8.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  2.09 | valid ppl     8.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  porestuwach thas ofiting dithed andlese of Trom a \n",
      "\n",
      "| epoch   5 |   100/   93 batches | lr 0.00 | loss  2.20 | ppl     8.99\n",
      "| epoch   5 |   200/   93 batches | lr 0.00 | loss  2.17 | ppl     8.72\n",
      "| epoch   5 |   300/   93 batches | lr 0.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   5 |   400/   93 batches | lr 0.00 | loss  2.17 | ppl     8.73\n",
      "| epoch   5 |   500/   93 batches | lr 0.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   5 |   600/   93 batches | lr 0.00 | loss  2.17 | ppl     8.75\n",
      "| epoch   5 |   700/   93 batches | lr 0.00 | loss  2.17 | ppl     8.76\n",
      "| epoch   5 |   800/   93 batches | lr 0.00 | loss  2.17 | ppl     8.77\n",
      "| epoch   5 |   900/   93 batches | lr 0.00 | loss  2.18 | ppl     8.82\n",
      "| epoch   5 |  1000/   93 batches | lr 0.00 | loss  2.18 | ppl     8.83\n",
      "| epoch   5 |  1100/   93 batches | lr 0.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   5 |  1200/   93 batches | lr 0.00 | loss  2.17 | ppl     8.75\n",
      "| epoch   5 |  1300/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  1400/   93 batches | lr 0.00 | loss  2.17 | ppl     8.75\n",
      "| epoch   5 |  1500/   93 batches | lr 0.00 | loss  2.17 | ppl     8.80\n",
      "| epoch   5 |  1600/   93 batches | lr 0.00 | loss  2.17 | ppl     8.80\n",
      "| epoch   5 |  1700/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  1800/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  1900/   93 batches | lr 0.00 | loss  2.18 | ppl     8.81\n",
      "| epoch   5 |  2000/   93 batches | lr 0.00 | loss  2.17 | ppl     8.74\n",
      "| epoch   5 |  2100/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  2200/   93 batches | lr 0.00 | loss  2.17 | ppl     8.76\n",
      "| epoch   5 |  2300/   93 batches | lr 0.00 | loss  2.18 | ppl     8.85\n",
      "| epoch   5 |  2400/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  2500/   93 batches | lr 0.00 | loss  2.18 | ppl     8.84\n",
      "| epoch   5 |  2600/   93 batches | lr 0.00 | loss  2.17 | ppl     8.80\n",
      "| epoch   5 |  2700/   93 batches | lr 0.00 | loss  2.17 | ppl     8.78\n",
      "| epoch   5 |  2800/   93 batches | lr 0.00 | loss  2.16 | ppl     8.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  2.09 | valid ppl     8.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " g meeping Draerbes . \" w sidutice ard <unk> the ha \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print('sample:\\n', generate(50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
